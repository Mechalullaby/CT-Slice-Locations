{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting CT Slice Locations\n",
    "---\n",
    "The primary purpose of this project is to assess the ability to use a matrix-based computational environment in the context of machine learning methods. In this project, `Python+NumPy` are mainly used, plus some functions in `SciPy`.\n",
    "\n",
    "The dataset used in this project is from the UCI machine learning repository. Some features have been extracted from slices of CT medical scans, and the task is predicting the location of a slice in the body from features of the scan (regression task). \n",
    "\n",
    "Source of the dataset: https://archive.ics.uci.edu/ml/datasets/Relative+location+of+CT+slices+on+axial+axis\n",
    "\n",
    "The patient IDs were removed from this version of the data, leaving 384 input features which were put in each of the “`X_...`” arrays. The corresponding CT scan slice location has been put in the “`y_...`” arrays. We shifted and scaled the “`y_...`” location values for the version of the data that you are using. The shift and scaling was chosen to make the training locations have zero mean and unit variance.\n",
    "\n",
    "The first 73 patients were put in the `_train` arrays, the next 12 in the `_val` arrays, and the final 12 in the `_test` arrays."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "In this section, we load the dataset, take a glance, and do some data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimension of X_train is (40754, 384)\n",
      "The dimension of X_val is (5785, 384)\n",
      "The dimension of X_test is (6961, 384)\n"
     ]
    }
   ],
   "source": [
    "print('The dimension of X_train is', X_train.shape)\n",
    "print('The dimension of X_val is', X_val.shape)\n",
    "print('The dimension of X_test is', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean for y_val is -0.2160085093241599\n",
      "The sd for y_val is 0.9814208579483531\n"
     ]
    }
   ],
   "source": [
    "print(f'The mean for y_val is {np.mean(y_val)}')\n",
    "print(f'The sd for y_val is {np.sqrt(np.var(y_val))}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we identify the features that take on the same value for every training example, and remove them in the three sets (training, validation and test set). Also, we identify identical columns in the training set and delete the later columns in the other three sets. \n",
    "\n",
    "In detail, we calculate the standard deviation of each column, and the column with 0 std is a constant feature. As for the detection of duplicate features, we sum up each column and identify the same numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant features:  [ 59  69 179 189 351]\n",
      "duplicate features:  [ 69  78  79 179 188 189 199 287 351 359]\n",
      "of which new to remove:  [ 78  79 188 199 287 359]\n",
      "all features removing:  [ 59  69  78  79 179 188 189 199 287 351 359]\n"
     ]
    }
   ],
   "source": [
    "# find the constant features\n",
    "mask1 = np.std(X_train, 0) > 0\n",
    "print('constant features: ', (mask1 != True).nonzero()[0])\n",
    "\n",
    "# Take the sum of the rows. If matches to 32 bit precision, columns\n",
    "# are probably the same. Yes, I could have been more careful here...\n",
    "rc = np.array(np.dot(np.ones(X_train.shape[0]), X_train), dtype=np.float32)\n",
    "_, idx = np.unique(rc, return_index=True)\n",
    "mask2 = np.tile(False, X_train.shape[1])\n",
    "mask2[idx] = True\n",
    "print('duplicate features: ', (mask2 != True).nonzero()[0])\n",
    "print('of which new to remove: ', ((mask2 != True) & mask1).nonzero()[0])\n",
    "\n",
    "mask = mask1 & mask2\n",
    "print('all features removing: ', (mask != True).nonzero()[0])\n",
    "\n",
    "X_train = X_train[:, mask]\n",
    "X_val = X_val[:, mask]\n",
    "X_test = X_test[:, mask]\n",
    "D = X_train.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "Using `numpy.linalg.lstsq`, we write a short function `fit_linreg(X, yy, alpha)` that fits the linear regression model $$f(\\textbf x;\\textbf w,b) = \\textbf w^{\\top}\\textbf x + b,$$ by minimizing the cost function: $$E(\\textbf w, b) = \\alpha\\textbf w^{\\top}\\textbf w + \\sum_n (f(\\textbf x^{(n)};\\textbf w,b) - y^{(n)})^2,$$ with regularization constant $\\alpha$. Fitting a bias parameter $b$ and incorporating the regularization constant can both be achieved by augmenting the original data arrays. A quick revision, suppose we have $$\\tilde{\\textbf y} = \\begin{bmatrix}\\textbf y \\\\ \\mathbf{0}_K\\end{bmatrix} \\quad \\tilde{\\Phi} = \\begin{bmatrix}\\Phi\\\\ \\sqrt{\\lambda}\\mathbb I_K\\end{bmatrix},$$ where $\\mathbf{0}_K$ is a vector of $K$ zeros, and $\\mathbb I_K$ is the $K\\times K$ identity matrix. Then\n",
    "$$\\begin{align*}\n",
    "    E(\\textbf w;\\,\\tilde{\\textbf y},\\tilde{\\Phi}) &= (\\tilde{\\textbf y} - \\tilde{\\Phi}\\textbf w)^\\top (\\tilde{\\textbf y} - \\tilde{\\Phi}\\textbf w)\\\\\n",
    "            &= (\\textbf y - \\Phi\\textbf w)^\\top (\\textbf y - \\Phi\\textbf w) + \\lambda\\textbf w^\\top\\textbf w\n",
    "            = E_\\lambda(\\textbf w;\\,\\textbf y,\\Phi).\n",
    "\\end{align*}$$\n",
    "\n",
    "Here, we use a data augmentation approach that maintains the numerical stability of the underlying `lstsq` solver, rather than a ‘normal equations’ approach. \n",
    "\n",
    "We use the constructed function to fit weights and a bias to `X_train` and `y_train`, with $\\alpha=30$. We also fit the same model with a gradient-based optimizer with the help of `fit_linreg_gradopt` in the support code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the regularized linear regression function\n",
    "def fit_linreg(X, yy, alpha):\n",
    "    n = X.shape[0]\n",
    "    # add a column of ones to X\n",
    "    X = np.hstack([X, np.ones(n).reshape(n,1)])\n",
    "\n",
    "    k = X.shape[1]\n",
    "    # add the regulation to X\n",
    "    X_tilde = np.vstack((X, np.sqrt(alpha)*np.eye(k)))\n",
    "\n",
    "    yy = np.append(yy,[0]*k) # for the regulation\n",
    "    return np.linalg.lstsq(X_tilde, yy, rcond=None)\n",
    "\n",
    "# compute the RMSE\n",
    "def rmse_linreg(fitmodel, X, yy):\n",
    "    w = fitmodel[0] # coeficients\n",
    "    n = X.shape[0]\n",
    "    X = np.hstack([X, np.ones(n).reshape(n,1)])\n",
    "    y_fit = np.dot(X, w)\n",
    "    return np.sqrt(sum((y_fit-yy)**2)/n)\n",
    "\n",
    "def rmse_gradopt(fitmodel, X, yy):\n",
    "    w = fitmodel[0] # coeficients\n",
    "    b = fitmodel[1]\n",
    "    n = X.shape[0]\n",
    "    y_fit = np.dot(X, w)+b\n",
    "    return np.sqrt(sum((y_fit-yy)**2)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ct_support_code import *\n",
    "fitmodel = fit_linreg(X_train, y_train, 30)\n",
    "gradopt = fit_linreg_gradopt(X_train, y_train, 30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the root-mean-square errors (RMSE) on the training and validation sets for the parameters fitted using both `fit_linreg` and the provided `fit_linreg_gradopt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For fit_linreg:\n",
      "The RMSE for training set is 0.35676698149487857\n",
      "The RMSE for validation set is 0.42292954946321265\n",
      "For fit_linreg_gradopt:\n",
      "The RMSE for training set is 0.35675714280747745\n",
      "The RMSE for validation set is 0.4230631666614895\n"
     ]
    }
   ],
   "source": [
    "print('For fit_linreg:')\n",
    "print(f'The RMSE for training set is {rmse_linreg(fitmodel, X_train, y_train)}')\n",
    "print(f'The RMSE for validation set is {rmse_linreg(fitmodel, X_val, y_val)}')\n",
    "print('For fit_linreg_gradopt:')\n",
    "print(f'The RMSE for training set is {rmse_gradopt(gradopt, X_train, y_train)}')\n",
    "print(f'The RMSE for validation set is {rmse_gradopt(gradopt, X_val, y_val)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE results are almost the same for the two models. They are similar because they both want to minimize the regularized least squares cost. However, `fit_linreg` finds the coeficients using matrix multiplication, `fit_linreg_gradopt` derives them by gradient descent, which result in the slight differences. The gradient solver may not have completely converged, but the least squares solver will only be accurate to numerical precision too. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invented classification tasks\n",
    "It is often easier to work with binary data than real-valued data: we don’t have to think so hard about how the values might be distributed, and how we might process them. Hence we invent some binary classification tasks, and fit these. \n",
    "\n",
    "We pick 20 positions within the range of training positions, and use each of these to define a classification task. \n",
    "\n",
    "The logistic regression cost function and gradients are provided in the function `logreg_cost`. It is analogous to the `linreg_cost` function for least-squares regression, which is used by the `fit_linreg_gradopt` function that we used earlier.\n",
    "\n",
    "We fit logistic regression to each of the 20 classification tasks above with $\\alpha=30$. Given a feature vector, we can now obtain 20 different probabilities, the predictions of the 20 logistic regression models. Then, we transform both the training and validation input matrices into new matrices with 20 columns, containing the probabilities from the 20 logistic regression models. \n",
    "\n",
    "Finally, we fit a regularized linear regression model ($\\alpha=30$) to the transformed 20-dimensional training set, and report the training and validation root mean square errors (RMSE) of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier 1 / 20\n",
      "Fitting classifier 2 / 20\n",
      "Fitting classifier 3 / 20\n",
      "Fitting classifier 4 / 20\n",
      "Fitting classifier 5 / 20\n",
      "Fitting classifier 6 / 20\n",
      "Fitting classifier 7 / 20\n",
      "Fitting classifier 8 / 20\n",
      "Fitting classifier 9 / 20\n",
      "Fitting classifier 10 / 20\n",
      "Fitting classifier 11 / 20\n",
      "Fitting classifier 12 / 20\n",
      "Fitting classifier 13 / 20\n",
      "Fitting classifier 14 / 20\n",
      "Fitting classifier 15 / 20\n",
      "Fitting classifier 16 / 20\n",
      "Fitting classifier 17 / 20\n",
      "Fitting classifier 18 / 20\n",
      "Fitting classifier 19 / 20\n",
      "Fitting classifier 20 / 20\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Fitting classifiers\n",
    "alpha = 30\n",
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "V_lr = np.zeros((K, D))\n",
    "bb_lr = np.zeros(K)\n",
    "for kk in range(K):\n",
    "    print('Fitting classifier %d / %d' % (kk+1, K))\n",
    "    labels = y_train > thresholds[kk]\n",
    "    args = (X_train, labels, alpha)\n",
    "    V_lr[kk,:], bb_lr[kk] = minimize_list(\n",
    "            logreg_cost, (np.zeros(D), np.array(0)), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_p_err = 0.154412\n",
      "val_p_err = 0.254248\n"
     ]
    }
   ],
   "source": [
    "# Using least squares solver:\n",
    "def fit_linreg(X, yy, alpha):\n",
    "    # Least squares problem, regularizing weights but not bias:\n",
    "    N, D = X.shape\n",
    "    X_bias = np.hstack([np.ones((N,1)), X])\n",
    "    X_reg = np.vstack([X_bias,\n",
    "            np.hstack([np.zeros((D,1)), np.sqrt(alpha)*np.eye(D)])])\n",
    "    yy_reg = np.hstack([yy, np.zeros(D)])\n",
    "    params = np.linalg.lstsq(X_reg, yy_reg, rcond=None)[0]\n",
    "    ww = params[1:]\n",
    "    bb = params[0]\n",
    "    return ww, bb\n",
    "\n",
    "# Function to report root-mean-square-error (RMSE) to use later:\n",
    "def rmse(y1,y2):\n",
    "    return np.sqrt(np.mean((y1-y2)**2))\n",
    "\n",
    "# Fitting dataset transformed using classifiers\n",
    "P_fn = lambda X: 1/(1 + np.exp(-(np.dot(X,V_lr.T) + bb_lr[None,:])))\n",
    "P_train = P_fn(X_train)\n",
    "P_val = P_fn(X_val)\n",
    "w_p, b_p = fit_linreg(P_train, y_train, alpha)\n",
    "pred_p_train = np.dot(P_train, w_p) + b_p\n",
    "pred_p_val = np.dot(P_val, w_p) + b_p\n",
    "print('train_p_err = %g' % rmse(pred_p_train, y_train))\n",
    "print('val_p_err = %g' % rmse(pred_p_val, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small neural network\n",
    "In the last section we fitted a small neural network. The logistic regression classifiers are sigmoidal hidden units, and a linear output unit predicts the outputs. However, we didn’t fit the parameters jointly to the obvious least squares cost function. A least squares cost function and gradients for this neural network are implemented in the `nn_cost` function provided.\n",
    "\n",
    "Here, we try fitting the neural network model to the training set, with a) a sensible random initialization of the parameters; b) the parameters initialized using the fits made in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14058177895782742\n",
      "0.13580955446486412\n"
     ]
    }
   ],
   "source": [
    "from scipy.linalg import cho_factor, cho_solve\n",
    "\n",
    "# a) Using random initialization of the parameters\n",
    "def fit_nn_random(X, yy, alpha): \n",
    "    args = (X, yy, alpha)\n",
    "    V = 0.1*np.random.randn(K, X.shape[1])/np.sqrt(X.shape[1])\n",
    "    bk = 0.1*np.random.randn(K)/np.sqrt(K)\n",
    "    ww = 0.1*np.random.randn(K)/np.sqrt(K)  \n",
    "    bb = 0.1*np.random.randn(1)\n",
    "    init = (ww, bb, V, bk)\n",
    "    ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "    return ww, bb, V, bk\n",
    "\n",
    "# b) Using parameters initialized using the fits made in Q3\n",
    "def fit_nn_logi(X, yy, alpha): \n",
    "    args = (X, yy, alpha)\n",
    "    V = w_hat_m.T\n",
    "    bk = b_hat_v\n",
    "    ww = w_train  \n",
    "    bb = b_train\n",
    "    init = (ww, bb, V, bk)\n",
    "    ww, bb, V, bk = minimize_list(nn_cost, init, args)\n",
    "    return ww, bb, V, bk\n",
    "\n",
    "# Prediction: \n",
    "n = X_train.shape[0]\n",
    "np.random.seed(0)\n",
    "ww, bb, V, bk = fit_nn_random(X = X_train, yy = y_train, alpha = 30)\n",
    "para = (ww, bb, V, bk)\n",
    "RMSE_random = np.sqrt(sum((nn_cost(para, X=X_train, yy=None, alpha=None) - y_train)**2)/n)\n",
    "www, bbb, VV, bkk = fit_nn_logi(X = X_train, yy = y_train, alpha = 30)\n",
    "para_logi = (www,bbb,VV,bkk)\n",
    "RMSE_logi = np.sqrt(sum((nn_cost(para_logi, X=X_train, yy=None, alpha=None) - y_train)**2)/n)\n",
    "\n",
    "print(RMSE_random)\n",
    "print(RMSE_logi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimisation\n",
    "A popular application area of Gaussian processes is Bayesian optimisation, where the uncertainty in the probabilistic model is used to guide the optimisation of a function. Here we will use Bayesian optimisation with Gaussian processes for choosing the regularisation parameter $\\alpha$.\n",
    "\n",
    "Gaussian processes are used to represent our belief about an unknown function. In this case, the function we are interested in is the neural network’s validation log root mean square error (log RMSE) as a function of the regularisation paramter $\\alpha$. In Bayesian optimisation, it is common to maximise the unknown function, so we will maximise the negative log RMSE.\n",
    "\n",
    "We start with a Gaussian process prior over this function. As we observe the actual log RMSEs for particular $\\alpha$’s we update our belief about the function by calculating the Gaussian process posterior.\n",
    "\n",
    "There are many popular acquisition functions in Bayesian optimisation. One example is the *probability of improvement*. Suppose we have observed $y^{(1)}$ to $y^{(n)}$ (here negative log RMSE at locations $\\alpha^{(1)}$ to $\\alpha^{(n)}$). Then the function takes the following form: $$\\notag\\mathit{PI}(\\alpha) = \\Phi\\left(\\frac{\\mu(\\alpha) - \\text{max}(y^{(1)},\\dots,y^{N})}{\\sigma(\\alpha)}\\right),$$ \n",
    "where $\\mu(\\alpha)$ is the Gaussian process posterior mean at location $\\alpha$, $\\sigma(\\alpha)$ is the posterior standard deviation at location $\\alpha$, $\\Phi$ denotes the cumulative density function of the Gaussian with mean 0 and variance 1. We pick the next regularization constant $\\alpha^{(N+1)}$ by maximizing the acquisition function: $$\\notag\\alpha^{(N+1)} = \\argmax_\\alpha\\mathit{PI}(\\alpha).$$\n",
    "We then evaluate our model for this regularization parameter and update our posterior about the unknown function that maps $\\alpha$ to negative log RMSE. We repeat the procedure multiple times and then pick the parameter that yielded the best performance $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ct_support_code import *\n",
    "import scipy.stats\n",
    "\n",
    "# trains the neural network for an alpha, returns the validation RMSE\n",
    "def train_nn_reg(X, yy, X_val, y_val, alpha):\n",
    "    ww, bb, V, bk = fit_nn_random(X, yy, alpha)\n",
    "    para = (ww, bb, V, bk)\n",
    "    y_fit = nn_cost(para, X=X_val)\n",
    "    return np.sqrt(sum((y_fit-y_val)**2)/len(y_fit))\n",
    "\n",
    "# function for generating y\n",
    "base = train_nn_reg(X_train, y_train, X_val, y_val, 30)\n",
    "def LRMSE(X, yy, X_val, y_val, alpha):\n",
    "    return np.log(base/train_nn_reg(X, yy, X_val, y_val, alpha))\n",
    "\n",
    "# select next alpha according to PoI\n",
    "def next_alpha(mu_star, cov_star, y_alpha):\n",
    "    PoI = scipy.stats.norm.pdf((mu_star - max(y_alpha))/np.sqrt(np.diag(cov_star)))\n",
    "    max_PoI = max(PoI)\n",
    "    max_alpha = X_star[np.where(PoI==max_PoI)[0][0]]\n",
    "    return max_PoI, max_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0, 50, 0.02)\n",
    "# Pick three alphas from this set for as training locations, \n",
    "ind = len(alphas)/4\n",
    "X_alpha = alphas.take([ind-1,ind*2-1,ind*3-1])\n",
    "# remaining locations as values for the acquisition function\n",
    "X_star = np.setdiff1d(alphas, X_alpha)\n",
    "\n",
    "# generate y for the picked alpha\n",
    "y_alpha = np.zeros(len(X_alpha)) \n",
    "for i in range(len(X_alpha)):\n",
    "    y_alpha[i] = LRMSE(X_train, y_train, X_val, y_val, X_alpha[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum probability of improvement is 0.3762439131308956, with alpha = 13.86, for iteration 1\n",
      "The maximum probability of improvement is 0.3622776781757357, with alpha = 9.08, for iteration 2\n",
      "The maximum probability of improvement is 0.34566890861340177, with alpha = 9.1, for iteration 3\n",
      "The maximum probability of improvement is 0.35452121459600816, with alpha = 9.06, for iteration 4\n",
      "The maximum probability of improvement is 0.3173822033001063, with alpha = 4.96, for iteration 5\n"
     ]
    }
   ],
   "source": [
    "# retrain, do five iterations\n",
    "iteration = 0\n",
    "while iteration < 5:\n",
    "    # calculate the posterior Gaussian Process\n",
    "    mu_star, cov_star = gp_post_par(X_star, X_alpha, y_alpha)\n",
    "    # select next alpha according to PoI\n",
    "    max_PoI, alpha_new = next_alpha(mu_star, cov_star, y_alpha)\n",
    "    iteration += 1\n",
    "    print(f'The maximum probability of improvement is {max_PoI}, with alpha = {alpha_new}, for iteration {iteration}')\n",
    "    X_alpha = np.append(X_alpha, alpha_new)\n",
    "    X_star = np.setdiff1d(alphas, X_alpha)\n",
    "    y_alpha = np.append(y_alpha, LRMSE(X_train, y_train, X_val, y_val, alpha_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha is 12.48, with its validation RMSE is 0.2582302787839227 and test RMSE is 0.2892353771241661\n"
     ]
    }
   ],
   "source": [
    "# report the best alpha\n",
    "best_alpha = X_alpha[np.where(y_alpha==max(y_alpha))[0][0]]\n",
    "\n",
    "val_RMSE = train_nn_reg(X_train, y_train, X_val, y_val, best_alpha)\n",
    "test_RMSE = train_nn_reg(X_train, y_train, X_test, y_test, best_alpha)\n",
    "\n",
    "print(f'The best alpha is {best_alpha}, with its validation RMSE is {val_RMSE} and test RMSE is {test_RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is improved with smaller validation and test RMSEs. The observation noise can come from the random selection of initial parameters in the neural network. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next\n",
    "From the description of the data set, we know that the features indicate bone structures and air inclusions. In detail, after dropping the columns in question 1, the first to the 231th columns denotes bone structures. We know that for each CT slice (y), the bone structure should have a special pattern (or say, special shape). Hence, we are wondering if we can reshape the feature columns into a matrix, and employ CNN. However, this method involves theory about CNN, also relating to complicate coding, which we can’t apply given limited time, but we thought this may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1394614055917868"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "LGBR = lgb.LGBMRegressor() \n",
    "LGBR.fit(X_train, y_train)\n",
    "y_pred = LGBR.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(sum((y_pred-y_test)**2)/n)\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8a05ca3558f2d51636bec583979faa97191f9d7382687da67a577b720c7dafe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
